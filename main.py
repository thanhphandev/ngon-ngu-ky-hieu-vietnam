import cv2
import mediapipe as mp
import sys
import time
import warnings
import streamlit as st

# Import c√°c module x·ª≠ l√Ω
from utils.feature_extraction import extract_features
from utils.strings import ExpressionHandler
from utils.tts import TextToSpeech
from utils.model import ASLClassificationModel
from utils.visualizer import Visualizer
from config import MODEL_NAME, MODEL_CONFIDENCE, PREDICTION_CONFIDENCE_THRESHOLD

# B·ªè qua c√°c c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt
warnings.filterwarnings("ignore")

# ==========================================
# 1. C·∫§U H√åNH GIAO DI·ªÜN STREAMLIT
# ==========================================
st.set_page_config(page_title="ASL Recognition Pro", layout="wide", page_icon="üñêÔ∏è")

st.markdown("""
    <style>
        .big-font {
            color: #e76f51 !important;
            font-size: 50px !important;
            font-weight: bold;
            border: 2px solid #fcbf49;
            border-radius: 10px;
            padding: 10px;
            text-align: center;
            background-color: #ffffff;
        }
        .stProgress > div > div > div > div {
            background-color: #2a9d8f;
        }
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. H√ÄM LOAD T√ÄI NGUY√äN
# ==========================================
@st.cache_resource
def load_ai_model():
    """Load model AI"""
    return ASLClassificationModel.load_model(f"models/{MODEL_NAME}")

@st.cache_resource
def load_visualizer():
    """Load c√¥ng c·ª• v·∫Ω"""
    return Visualizer()

# Kh·ªüi t·∫°o
try:
    model = load_ai_model()
    visualizer = load_visualizer()
except Exception as e:
    st.error(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o: {e}")
    st.stop()

# ==========================================
# 3. SIDEBAR & C·∫§U H√åNH
# ==========================================
st.sidebar.title("üîß B·∫£ng ƒêi·ªÅu Khi·ªÉn")

run_camera = st.sidebar.checkbox("üì∑ B·∫≠t Camera", value=True)

st.sidebar.markdown("---")
st.sidebar.subheader("‚öôÔ∏è C·∫•u h√¨nh MediaPipe")
detection_confidence = st.sidebar.slider("ƒê·ªô nh·∫°y ph√°t hi·ªán (Detection)", 0.0, 1.0, MODEL_CONFIDENCE, 0.05)
tracking_confidence = st.sidebar.slider("ƒê·ªô nh·∫°y theo d√µi (Tracking)", 0.0, 1.0, MODEL_CONFIDENCE, 0.05)

st.sidebar.markdown("---")
st.sidebar.subheader("üîä Gi·ªçng n√≥i (TTS)")
tts_enabled = st.sidebar.checkbox("B·∫≠t ƒë·ªçc k·∫øt qu·∫£", value=False)
tts_engine_choice = st.sidebar.selectbox("C√¥ng c·ª• ƒë·ªçc", ["pyttsx3 (Offline)", "gTTS (Vietnamese, Online)"], index=0)
min_interval = st.sidebar.slider("Kho·∫£ng c√°ch ƒë·ªçc (gi√¢y). Khuy·∫øn ngh·ªã 2 gi√¢y", 1.0, 5.0, 2.0, 0.5)

# X·ª≠ l√Ω TTS Session
if 'tts' not in st.session_state:
    st.session_state.tts = None
    st.session_state.tts_engine = None

desired_engine = 'pyttsx3' if 'pyttsx3' in tts_engine_choice else 'gtts'

if tts_enabled:
    if st.session_state.tts is None or st.session_state.tts_engine != desired_engine:
        try:
            with st.spinner("ƒêang kh·ªüi t·∫°o gi·ªçng n√≥i..."):
                st.session_state.tts = TextToSpeech(engine=desired_engine, lang='vi')
                st.session_state.tts_engine = desired_engine
        except Exception as e:
            st.sidebar.error(f"L·ªói TTS: {e}")
            tts_enabled = False
elif not tts_enabled and st.session_state.tts is not None:
    st.session_state.tts = None

# ==========================================
# 4. GIAO DI·ªÜN CH√çNH
# ==========================================
col1, col2 = st.columns([3, 2])

with col1:
    st.markdown("### üé• Camera Feed")
    video_placeholder = st.empty()

with col2:
    st.markdown("### üß† Ph√¢n t√≠ch AI")
    prediction_placeholder = st.empty()
    
    st.markdown("#### ƒê·ªô tin c·∫≠y (Confidence)")
    confidence_bar = st.progress(0)
    confidence_text = st.empty()
    
    st.markdown("---")
    fps_display = st.empty()

# ==========================================
# 5. LOGIC X·ª¨ L√ù CAMERA (LOOP)
# ==========================================
if run_camera:
    mp_face_mesh = mp.solutions.face_mesh
    mp_hands = mp.solutions.hands
    
    cap = cv2.VideoCapture(0)
    expression_handler = ExpressionHandler()
    prev_time = 0

    with mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=detection_confidence,
            min_tracking_confidence=tracking_confidence) as face_mesh, \
         mp_hands.Hands(
            max_num_hands=2,
            min_detection_confidence=detection_confidence,
            min_tracking_confidence=tracking_confidence) as hands:

        while cap.isOpened() and run_camera:
            success, image = cap.read()
            if not success:
                st.warning("Kh√¥ng t√¨m th·∫•y camera.")
                break

            # T√≠nh FPS
            curr_time = time.time()
            fps = 1 / (curr_time - prev_time) if (curr_time - prev_time) > 0 else 0
            prev_time = curr_time
            fps_display.metric("FPS", f"{int(fps)}")

            # X·ª≠ l√Ω h√¨nh ·∫£nh
            image.flags.writeable = False
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # 1. Detect
            face_results = face_mesh.process(image)
            hand_results = hands.process(image)

            # 2. V·∫Ω (S·ª≠ d·ª•ng Visualizer m·ªõi)
            image.flags.writeable = True
            image = visualizer.draw_landmarks(image, face_results, hand_results)

            # 3. D·ª± ƒëo√°n
            # Ch·ªâ d·ª± ƒëo√°n n·∫øu ph√°t hi·ªán ƒë∆∞·ª£c tay ho·∫∑c m·∫∑t
            if face_results.multi_face_landmarks or hand_results.multi_hand_landmarks:
                try:
                    feature = extract_features(mp_hands, face_results, hand_results)
                    
                    # D√πng h√†m m·ªõi predict_with_confidence
                    label, confidence = model.predict_with_confidence(feature)
                    
                    # --- LOGIC M·ªöI: DUAL CONFIDENCE THRESHOLD ---
                    # N·∫øu ƒë·ªô tin c·∫≠y th·∫•p h∆°n ng∆∞·ª°ng cho ph√©p -> Coi l√† "binh_thuong" (Idle)
                    if confidence < PREDICTION_CONFIDENCE_THRESHOLD:
                        label = "binh_thuong"
                    
                    expression_handler.receive(label)
                    ui_text = expression_handler.get_message()

                    # C·∫≠p nh·∫≠t UI
                    prediction_placeholder.markdown(f'<div class="big-font">{ui_text}</div>', unsafe_allow_html=True)
                    
                    # C·∫≠p nh·∫≠t thanh Confidence
                    confidence_bar.progress(float(confidence))
                    confidence_text.text(f"ƒê·ªô ch√≠nh x√°c: {confidence*100:.1f}%")

                    # ƒê·ªçc gi·ªçng n√≥i
                    if tts_enabled and st.session_state.tts:
                        speech_text = expression_handler.get_speech_message()

                        # Do not speak if label is "binh_thuong"
                        if label != "binh_thuong" and speech_text.strip():
                            st.session_state.tts.speak_if_allowed(speech_text, min_interval=min_interval)

                except Exception as e:
                    # print(f"Error: {e}") # Debug only
                    pass
            else:
                # N·∫øu kh√¥ng c√≥ landmarks (kh√¥ng ng∆∞·ªùi, kh√¥ng tay), reset UI
                prediction_placeholder.markdown(f'<div class="big-font">...</div>', unsafe_allow_html=True)
                confidence_bar.progress(0)
                confidence_text.text("ƒêang ch·ªù t√≠n hi·ªáu...")

            # Hi·ªÉn th·ªã
            video_placeholder.image(image, channels="RGB", use_column_width=True)

    cap.release()
    # cv2.destroyAllWindows() # Kh√¥ng c·∫ßn thi·∫øt tr√™n Streamlit Cloud v√† g√¢y l·ªói v·ªõi headless
else:
    st.info("üëã H√£y b·∫≠t camera ƒë·ªÉ b·∫Øt ƒë·∫ßu tr·∫£i nghi·ªám.")